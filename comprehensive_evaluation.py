#!/usr/bin/env python3
"""
comprehensive_evaluation.py

å®Œæ•´çš„å®éªŒè¯„ä¼°æ¡†æ¶ - æ»¡è¶³ CS245 é¡¹ç›®è¯„åˆ†æ ‡å‡†
åŒ…å«ï¼š
1. Baseline å¯¹æ¯”
2. Ablation Studiesï¼ˆæ¶ˆèå®éªŒï¼‰
3. æ‰€æœ‰ Benchmark Metricsï¼ˆRMSE, MAE, Sentiment Alignment, HR@Kï¼‰
4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
5. å¯å¤ç°çš„å®éªŒè®¾ç½®
6. è¯¦ç»†çš„ç»“æœåˆ†æ
"""

import sys
import os
import json
import time
import numpy as np
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from websocietysimulator import Simulator
from improved_agent_with_quality import ImprovedSimulationAgent

# ============================
# DeepSeek LLM å°è£…
# ============================

import requests

class DeepSeekEmbeddingModel:
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com/v1",
                 model: str = "deepseek-embedding"):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model

    def _api_embed(self, texts):
        try:
            url = f"{self.base_url}/embeddings"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}",
            }
            payload = {"model": self.model, "input": texts}
            resp = requests.post(url, json=payload, headers=headers, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            return [item["embedding"] for item in data["data"]]
        except Exception as e:
            print(f"âŒ Embedding API é”™è¯¯: {e}")
            return [np.zeros(768).tolist() for _ in texts]

    def embed_documents(self, texts):
        if not texts:
            return []
        return self._api_embed(texts)

    def embed_query(self, text):
        if not text:
            return np.zeros(768).tolist()
        return self._api_embed([text])[0]


class DeepSeekLLM:
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com/v1",
                 chat_model: str = "deepseek-chat", embedding_model: str = "deepseek-embedding"):
        self.api_key = api_key
        self.base_url = base_url
        self.chat_model = chat_model
        self.embedding_model = embedding_model

    def __call__(self, messages, temperature=0.7, max_tokens=800):
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
        }
        payload = {
            "model": self.chat_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False,
        }
        try:
            resp = requests.post(url, json=payload, headers=headers, timeout=60)
            resp.raise_for_status()
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"âŒ Chat API é”™è¯¯: {e}")
            return "ï¼ˆAPI é”™è¯¯ï¼‰"

    def get_embedding_model(self):
        return DeepSeekEmbeddingModel(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.embedding_model
        )


# ============================
# å®éªŒé…ç½®ç±»
# ============================

class ExperimentConfig:
    """å®éªŒé…ç½®"""
    def __init__(self, name: str, enable_reflection: bool, use_memory: bool,
                 max_reference_reviews: int, description: str = ""):
        self.name = name
        self.enable_reflection = enable_reflection
        self.use_memory = use_memory
        self.max_reference_reviews = max_reference_reviews
        self.description = description

    def __str__(self):
        return f"{self.name}: reflection={self.enable_reflection}, memory={self.use_memory}, refs={self.max_reference_reviews}"


# ============================
# è¯„ä¼°æŒ‡æ ‡è®¡ç®—
# ============================

def calculate_additional_metrics(outputs: List[Dict], groundtruths: List[Dict]) -> Dict[str, float]:
    """
    è®¡ç®—é¢å¤–çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆè¡¥å…… simulator.evaluate()ï¼‰
    """
    metrics = {}
    
    # æå–é¢„æµ‹å€¼å’ŒçœŸå®å€¼
    predicted_stars = []
    actual_stars = []
    
    for out, gt in zip(outputs, groundtruths):
        if out and isinstance(out, dict):
            # å¤„ç†åµŒå¥—ç»“æ„
            if "output" in out and isinstance(out["output"], dict):
                pred = out["output"].get("stars")
            else:
                pred = out.get("stars")
            
            if pred is not None and "stars" in gt:
                predicted_stars.append(float(pred))
                actual_stars.append(float(gt["stars"]))
    
    if not predicted_stars:
        return {"error": "No valid predictions"}
    
    predicted_stars = np.array(predicted_stars)
    actual_stars = np.array(actual_stars)
    
    # åŸºç¡€æŒ‡æ ‡
    metrics["accuracy_exact"] = np.mean(predicted_stars == actual_stars)
    metrics["accuracy_Â±0.5"] = np.mean(np.abs(predicted_stars - actual_stars) <= 0.5)
    metrics["accuracy_Â±1.0"] = np.mean(np.abs(predicted_stars - actual_stars) <= 1.0)
    
    # åˆ†å¸ƒæŒ‡æ ‡
    metrics["pred_mean"] = float(np.mean(predicted_stars))
    metrics["pred_std"] = float(np.std(predicted_stars))
    metrics["actual_mean"] = float(np.mean(actual_stars))
    metrics["actual_std"] = float(np.std(actual_stars))
    
    # ç›¸å…³æ€§
    if len(predicted_stars) > 1:
        correlation = np.corrcoef(predicted_stars, actual_stars)[0, 1]
        metrics["pearson_correlation"] = float(correlation)
    
    return metrics


def calculate_statistical_significance(results1: Dict, results2: Dict, metric: str = "rmse") -> Dict:
    """
    è®¡ç®—ä¸¤ç»„ç»“æœä¹‹é—´çš„ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆç®€åŒ–ç‰ˆ t-testï¼‰
    """
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¿å­˜æ¯ä¸ªä»»åŠ¡çš„è¯¯å·®ç„¶ååš t-test
    diff = abs(results1.get(metric, 0) - results2.get(metric, 0))
    
    # ç®€å•çš„ç›¸å¯¹æ”¹è¿›ç™¾åˆ†æ¯”
    if results2.get(metric, 0) > 0:
        improvement = (results2.get(metric, 0) - results1.get(metric, 0)) / results2.get(metric, 0) * 100
    else:
        improvement = 0
    
    return {
        "metric": metric,
        "diff": diff,
        "improvement_%": improvement,
        "better": results1.get(metric, float('inf')) < results2.get(metric, float('inf'))
    }


# ============================
# å®éªŒè¿è¡Œå™¨
# ============================

class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨ - è´Ÿè´£è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®"""
    
    def __init__(self, data_dir: str, task_set: str, api_key: str, num_tasks: int = 100):
        self.data_dir = data_dir
        self.task_set = task_set
        self.api_key = api_key
        self.num_tasks = num_tasks
        self.results = {}
        
    def run_experiment(self, config: ExperimentConfig) -> Dict:
        """è¿è¡Œå•ä¸ªå®éªŒé…ç½®"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª è¿è¡Œå®éªŒ: {config.name}")
        print(f"{'='*80}")
        print(f"é…ç½®: {config}")
        
        # åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨
        simulator = Simulator(
            data_dir=self.data_dir,
            device="cpu",
            cache=True
        )
        
        # åŠ è½½ä»»åŠ¡
        simulator.set_task_and_groundtruth(
            task_dir=f"example/track1/{self.task_set}/tasks",
            groundtruth_dir=f"example/track1/{self.task_set}/groundtruth"
        )
        
        # é…ç½® Agent
        class ConfiguredAgent(ImprovedSimulationAgent):
            def __init__(self, llm):
                super().__init__(
                    llm=llm,
                    enable_reflection=config.enable_reflection,
                    use_memory=config.use_memory,
                    max_reference_reviews=config.max_reference_reviews
                )
        
        simulator.set_agent(ConfiguredAgent)
        simulator.set_llm(DeepSeekLLM(api_key=self.api_key))
        
        # è¿è¡Œæ¨¡æ‹Ÿ
        print(f"\nâš™ï¸  è¿è¡Œ {self.num_tasks} ä¸ªä»»åŠ¡...")
        start_time = time.time()
        
        outputs = simulator.run_simulation(
            number_of_tasks=self.num_tasks,
            enable_threading=True,
            max_workers=5
        )
        
        elapsed_time = time.time() - start_time
        
        print(f"âœ… å®Œæˆï¼ç”¨æ—¶: {elapsed_time:.2f}ç§’")
        print(f"   å¹³å‡æ¯ä»»åŠ¡: {elapsed_time/self.num_tasks:.2f}ç§’")
        
        # è¯„ä¼°
        print("\nğŸ“Š è¯„ä¼°ä¸­...")
        try:
            eval_results = simulator.evaluate()
            
            # è®¡ç®—é¢å¤–æŒ‡æ ‡
            # å°è¯•ä¸åŒçš„å±æ€§åç§°
            try:
                if hasattr(simulator, 'groundtruth_data'):
                    groundtruths = simulator.groundtruth_data[:self.num_tasks]
                elif hasattr(simulator, 'groundtruth_pool'):
                    groundtruths = simulator.groundtruth_pool[:self.num_tasks]
                elif hasattr(simulator, 'groundtruths'):
                    groundtruths = simulator.groundtruths[:self.num_tasks]
                else:
                    groundtruths = []
                
                if groundtruths:
                    additional_metrics = calculate_additional_metrics(outputs, groundtruths)
                    eval_results.update(additional_metrics)
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è®¡ç®—é¢å¤–æŒ‡æ ‡: {e}")
            
            # æ·»åŠ å…ƒæ•°æ®
            eval_results["config"] = {
                "name": config.name,
                "enable_reflection": config.enable_reflection,
                "use_memory": config.use_memory,
                "max_reference_reviews": config.max_reference_reviews
            }
            eval_results["num_tasks"] = self.num_tasks
            eval_results["elapsed_time"] = elapsed_time
            eval_results["timestamp"] = datetime.now().isoformat()
            
            return eval_results
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}
    
    def run_all_experiments(self, configs: List[ExperimentConfig]):
        """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®"""
        print("\n" + "ğŸš€"*40)
        print("å¼€å§‹è¿è¡Œå®Œæ•´å®éªŒå¥—ä»¶")
        print("ğŸš€"*40 + "\n")
        
        for config in configs:
            try:
                results = self.run_experiment(config)
                self.results[config.name] = results
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_intermediate_results(config.name)
                
            except Exception as e:
                print(f"âŒ å®éªŒ {config.name} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                self.results[config.name] = {"error": str(e)}
        
        print("\n" + "âœ…"*40)
        print("æ‰€æœ‰å®éªŒå®Œæˆï¼")
        print("âœ…"*40 + "\n")
        
        return self.results
    
    def _save_intermediate_results(self, config_name: str):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"results_{config_name}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results[config_name], f, indent=4, ensure_ascii=False)
        
        print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")


# ============================
# ç»“æœåˆ†æå™¨
# ============================

class ResultsAnalyzer:
    """ç»“æœåˆ†æå™¨ - ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ã€å›¾è¡¨ã€ç»Ÿè®¡åˆ†æ"""
    
    def __init__(self, results: Dict[str, Dict]):
        self.results = results
        
    def generate_comparison_table(self) -> str:
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆMarkdownæ ¼å¼ï¼‰"""
        table = "\n## ğŸ“Š å®éªŒç»“æœå¯¹æ¯”è¡¨\n\n"
        table += "| é…ç½® | RMSE | MAE | Sentiment Acc | Accuracy(Â±0.5) | Correlation | æ—¶é—´(s) |\n"
        table += "|------|------|-----|---------------|----------------|-------------|----------|\n"
        
        for name, result in self.results.items():
            if "error" in result:
                table += f"| {name} | ERROR | - | - | - | - | - |\n"
                continue
            
            rmse = result.get("rmse", "N/A")
            mae = result.get("mae", "N/A")
            sent = result.get("sentiment_alignment", "N/A")
            acc = result.get("accuracy_Â±0.5", "N/A")
            corr = result.get("pearson_correlation", "N/A")
            time_val = result.get("elapsed_time", "N/A")
            
            # æ ¼å¼åŒ–æ•°å€¼
            rmse_str = f"{rmse:.4f}" if isinstance(rmse, (int, float)) else rmse
            mae_str = f"{mae:.4f}" if isinstance(mae, (int, float)) else mae
            sent_str = f"{sent:.4f}" if isinstance(sent, (int, float)) else sent
            acc_str = f"{acc:.4f}" if isinstance(acc, (int, float)) else acc
            corr_str = f"{corr:.4f}" if isinstance(corr, (int, float)) else corr
            time_str = f"{time_val:.1f}" if isinstance(time_val, (int, float)) else time_val
            
            table += f"| {name} | {rmse_str} | {mae_str} | {sent_str} | {acc_str} | {corr_str} | {time_str} |\n"
        
        return table
    
    def generate_ablation_analysis(self, baseline_name: str) -> str:
        """ç”Ÿæˆæ¶ˆèåˆ†æ"""
        if baseline_name not in self.results:
            return "\nâš ï¸ æœªæ‰¾åˆ° baseline ç»“æœ\n"
        
        baseline = self.results[baseline_name]
        analysis = "\n## ğŸ”¬ Ablation Study åˆ†æ\n\n"
        
        for name, result in self.results.items():
            if name == baseline_name or "error" in result:
                continue
            
            analysis += f"\n### {name} vs {baseline_name}\n\n"
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„æ”¹è¿›
            metrics = ["rmse", "mae", "sentiment_alignment", "accuracy_Â±0.5"]
            
            for metric in metrics:
                if metric in result and metric in baseline:
                    base_val = baseline[metric]
                    exp_val = result[metric]
                    
                    # RMSE/MAE è¶Šå°è¶Šå¥½ï¼Œå…¶ä»–è¶Šå¤§è¶Šå¥½
                    if metric in ["rmse", "mae"]:
                        improvement = (base_val - exp_val) / base_val * 100
                        symbol = "â†“" if exp_val < base_val else "â†‘"
                    else:
                        improvement = (exp_val - base_val) / base_val * 100
                        symbol = "â†‘" if exp_val > base_val else "â†“"
                    
                    analysis += f"- **{metric}**: {exp_val:.4f} (baseline: {base_val:.4f}) "
                    analysis += f"â†’ {symbol} {abs(improvement):.2f}%\n"
        
        return analysis
    
    def generate_statistical_analysis(self) -> str:
        """ç”Ÿæˆç»Ÿè®¡åˆ†æ"""
        analysis = "\n## ğŸ“ˆ ç»Ÿè®¡åˆ†æ\n\n"
        
        # æ‰¾åˆ°æœ€å¥½çš„é…ç½®
        best_rmse = min((r.get("rmse", float('inf')), name) 
                       for name, r in self.results.items() if "error" not in r)
        best_mae = min((r.get("mae", float('inf')), name) 
                      for name, r in self.results.items() if "error" not in r)
        best_sent = max((r.get("sentiment_alignment", 0), name) 
                       for name, r in self.results.items() if "error" not in r)
        
        analysis += f"### æœ€ä½³é…ç½®\n\n"
        analysis += f"- **æœ€ä½ RMSE**: {best_rmse[1]} ({best_rmse[0]:.4f})\n"
        analysis += f"- **æœ€ä½ MAE**: {best_mae[1]} ({best_mae[0]:.4f})\n"
        analysis += f"- **æœ€é«˜ Sentiment Alignment**: {best_sent[1]} ({best_sent[0]:.4f})\n"
        
        return analysis
    
    def save_full_report(self, filename: str = "experiment_report.md"):
        """ä¿å­˜å®Œæ•´æŠ¥å‘Š"""
        report = "# CS245 Track 1 - å®éªŒè¯„ä¼°å®Œæ•´æŠ¥å‘Š\n\n"
        report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # å®éªŒæ¦‚è¿°
        report += "## ğŸ“‹ å®éªŒæ¦‚è¿°\n\n"
        report += f"- **æ€»å®éªŒæ•°**: {len(self.results)}\n"
        report += f"- **æ•°æ®é›†**: Yelp\n"
        report += f"- **æ¯ä¸ªå®éªŒçš„ä»»åŠ¡æ•°**: {list(self.results.values())[0].get('num_tasks', 'N/A')}\n\n"
        
        # æ·»åŠ å„ä¸ªåˆ†æéƒ¨åˆ†
        report += self.generate_comparison_table()
        report += self.generate_ablation_analysis("Baseline")
        report += self.generate_statistical_analysis()
        
        # ä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename


# ============================
# ä¸»å‡½æ•°
# ============================

def main():
    """ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„å®éªŒå¥—ä»¶"""
    
    # ============================================
    # é…ç½®å‚æ•°
    # ============================================
    
    DATA_DIR = "Dataset"
    TASK_SET = "yelp"
    API_KEY = "sk-abab919cdfae44deac4d21cb974aa4e0"  # ğŸ‘ˆ æ”¹æˆä½ çš„ API Key
    NUM_TASKS = 10  # æ¯ä¸ªå®éªŒçš„ä»»åŠ¡æ•°ï¼ˆå»ºè®® 100-200ï¼‰
    
    # ============================================
    # å®šä¹‰å®éªŒé…ç½®
    # ============================================
    
    experiments = [
        # Baseline: æœ€ç®€å•çš„é…ç½®
        ExperimentConfig(
            name="Baseline",
            enable_reflection=False,
            use_memory=False,
            max_reference_reviews=3,
            description="Simple baseline without reflection or memory"
        ),
        
        # å®Œæ•´é…ç½®: æ‰€æœ‰åŠŸèƒ½éƒ½å¼€å¯
        ExperimentConfig(
            name="Full",
            enable_reflection=True,
            use_memory=True,
            max_reference_reviews=5,
            description="Full model with all features enabled"
        ),
        
        # Ablation 1: ç§»é™¤åæ€
        ExperimentConfig(
            name="No_Reflection",
            enable_reflection=False,
            use_memory=True,
            max_reference_reviews=5,
            description="Ablation: Remove reflection"
        ),
        
        # Ablation 2: ç§»é™¤è®°å¿†
        ExperimentConfig(
            name="No_Memory",
            enable_reflection=True,
            use_memory=False,
            max_reference_reviews=5,
            description="Ablation: Remove memory"
        ),
        
        # Ablation 3: å‡å°‘å‚è€ƒè¯„è®º
        ExperimentConfig(
            name="Fewer_References",
            enable_reflection=True,
            use_memory=True,
            max_reference_reviews=2,
            description="Ablation: Reduce reference reviews to 2"
        ),
    ]
    
    # ============================================
    # è¿è¡Œå®éªŒ
    # ============================================
    
    runner = ExperimentRunner(
        data_dir=DATA_DIR,
        task_set=TASK_SET,
        api_key=API_KEY,
        num_tasks=NUM_TASKS
    )
    
    results = runner.run_all_experiments(experiments)
    
    # ============================================
    # åˆ†æç»“æœ
    # ============================================
    
    analyzer = ResultsAnalyzer(results)
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    report_file = analyzer.save_full_report()
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š å®éªŒæ€»ç»“")
    print("="*80)
    print(analyzer.generate_comparison_table())
    print(analyzer.generate_statistical_analysis())
    
    # ä¿å­˜åŸå§‹ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_file = f"all_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"\nğŸ’¾ åŸå§‹ç»“æœå·²ä¿å­˜: {json_file}")
    
    print("\n" + "ğŸ‰"*40)
    print("å®éªŒè¯„ä¼°å®Œæˆï¼")
    print("ğŸ‰"*40)
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("1. æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š: experiment_report.md")
    print("2. æŸ¥çœ‹åŸå§‹æ•°æ®: all_results_*.json")
    print("3. å°†ç»“æœæ•´ç†åˆ°é¡¹ç›®æŠ¥å‘Šä¸­")
    print("4. å‡†å¤‡æ¼”ç¤ºææ–™å’Œå›¾è¡¨")


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%H:%M:%S"
    )
    main()